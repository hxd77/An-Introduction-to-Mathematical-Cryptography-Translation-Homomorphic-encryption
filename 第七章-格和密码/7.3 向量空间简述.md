## 7.3 向量空间的简要回顾

> 这一节对应教材的 7.3.

向量空间有广义的定义，不过本章我们考虑在 $\mathbb{R}^m$ 上的向量空间。



**向量空间(Vector Spaces).** 一个向量空间 $V$ 是 $\mathbb{R}^m$ 的一个子集合，满足性质：

$$
\alpha_1 v_1+\alpha_2 v_2 \in V,\quad\text{for all}\ v_1,v_2 \in V \ \text{and all}\ \alpha_1,\alpha_2 \in \mathbb{R}\\
$$


即，一个向量空间 $V$ 是 $\mathbb{R}^m$ 的一个子集合，满足加法和取自 $\mathbb{R}$ 上元素的标量乘法封闭。



**线性组合(Linear Combinations).** 令 $v_1,v_2,\dots,v_k \in V$. $v_1,v_2,\dots,v_k \in V$ 的一个线性组合为一个有着如下形式的向量：

$$
w=\alpha_1 v_1+\alpha_2 v_2+\dots +\alpha_k v_k \quad with\ \alpha_1, \dots, \alpha_k \in \mathbb{R}\\
$$


**所有这样的线性组合向量构成的集合**：
$$
\{\alpha_1 v_1+\alpha_2 v_2+\dots +\alpha_k v_k :\ \alpha_1, \dots, \alpha_k \in \mathbb{R}\}\\
$$


称为 $\{v_1,v_2,\dots ,v_k \}$ 的一个扩张（span）.



**线性无关(Independence).** 称一组向量 $v_1,v_2,\dots,v_k \in V$ 是线性无关（linearly independent）当且仅当：

$$
\alpha_1 v_1+\alpha_2 v_2+\dots +\alpha_k v_k=0\\
$$


时，$\alpha_1=\alpha_2=\dots=\alpha_k=0$. 而线性相关则是上式成立时，至少有一个 $\alpha_i$ 非0.



**基底(Bases).** $V$ 的一个基底是一组线性无关向量 $v_1,v_2,\dots,v_n$ 能够扩张为整个 $V$. 即对于每一个向量 $w \in V$ , 都存在唯一的实数 $\alpha_1, \dots, \alpha_n \in \mathbb{R}$, 使得 $w$ 可以表示为：

$$
w=\alpha_1 v_1+\alpha_2 v_2+\dots +\alpha_n v_n\\
$$




**Proposition 7.11.** 令 $V \subset \mathbb{R}^m$ 是一个向量空间

1.  存在一个 $V$ 的基底
2.  $V$ 的任何两个基底都有相同数量的元素。基底中元素（向量）的个数称为向量空间的维数(dimension)
3.  令 $v_1,v_2,\dots,v_n$ 是 $V$ 的一个基底，$w_1,w_2,\dots,w_n$ 是 $V$ 中 $n$ 个向量。则 $w_j$ 可以写为 $v_i$ 的线性组合：  
    $$
    w_1 = a_{11}v_1 + a_{12}v_2+\dots+a_{1n}v_n,\\w_2 = a_{21}v_1 + a_{22}v_2+\dots+a_{2n}v_n,\\\vdots \\w_n = a_{n1}v_1 + a_{n2}v_2+\dots+a_{nn}v_n,\\
    $$
    
    则 $w_1,w_2,\dots,w_n$ 也是 $V$ 的一组基当且仅当以下矩阵的行列式不等于0（因为$det(A)\neq0$说明向量$w_j$线性无关，并且$A$可以用向量$v_i$来进行表示）：  
    $$
    \begin{pmatrix}{\alpha_{11}}&{\alpha_{12}}&{\cdots}&{\alpha_{1n}}\\{\alpha_{21}}&{\alpha_{22}}&{\cdots}&{\alpha_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{\alpha_{n1}}&{\alpha_{n2}}&{\cdots}&{\alpha_{nn}}\\\end{pmatrix}\\
    $$
    

下面我们将介绍如何衡量向量的长度（lengths） 和角度（angles）. 这与点积（dot product）和 Euclidean norm（即 $L_2$ 范式）有关.



**Definition.** 令 $v,w \in V \subset \mathbb{R}^m$, 并用坐标分别表示：

$$
v=(x_1,\dots,x_m)\ \text{and}\ w = (y_1, \dots, y_m)\\
$$


$v$ 与 $w$ 的内积为：

$$
v\cdot w = x_1y_1 + \dots+x_my_m\\
$$


我们说 $v$ 与 $w$ 是正交的（orthogonal）如果 $v\cdot w = 0$.

至于长度，或者说是 Euclidean norm 为：

$$
\left\lVert v \right\rVert = \sqrt{x^{2}_1+x^{2}_2+\dots +x^{2}_m}\\
$$


注意到点积和范数之间有以下关系：

$$
v\cdot v =\left\lVert v \right\rVert^2\\
$$




**Proposition 7.12.** 令 $v,w \in V \subset \mathbb{R}^m$.

1.  令 $\theta$ 表示 $v$ 与 $w$ 之间的角度。我们将向量 $v$ 和 $w$ 的起点放在原点 0，则  
    $$
    v\cdot w =\left\lVert v \right\rVert \left\lVert w \right\rVert \cos(\theta)\tag{7.6}
    $$
    
2.  (Cauchy-Schwarz 不等式)  
    $$
    \lvert v\cdot w \rvert \leq \left\lVert v \right\rVert \left\lVert w \right\rVert \tag{7.7}
    $$
    



**_Proof._** 命题 1 看简单的线性代数便可证明。当命题 1成立时，Cauchy-Schwarz 不等式立刻就能得到。但我们这里给出一个更直接的证明。

-   当 $w=0$ 时，直接成立，以下考虑 $w\neq 0$.
-   考虑函数：  
    $$
    \begin{align}f(t) = \left\lVert v -tw\right\rVert^2 &=(v-tw)\cdot(v-tw)\\&=v\cdot v -2tv\cdot w +t^2 w\cdot w \\&=\left\lVert w\right\rVert^2 \cdot t^2 -2v\cdot w \cdot t +\left\lVert v \right\rVert^2\end{align}\\
    $$
    
    对于 $\forall t \in \mathbb{R}$, 都有$f(t) \geq 0$, 因此我们取其最小值, 即当 $t=\frac{v\cdot w}{\left\lVert w \right\rVert^2}$ 时：  
    $$
    f(\frac{v\cdot w}{\left\lVert w \right\rVert^2})=\left\lVert v\right\rVert^2 - \frac{(v\cdot w)^2}{\left\lVert w \right\rVert^2} \geq 0\\
    $$
    
    化简后开根号，即可证得。



**Definition.** 一个向量空间 $V$ 的正交基（orthogonal basis）是一组基满足：

$$
v_i \cdot v_j = 0 \quad \forall i \neq j\\
$$


如果额外满足$\forall i,\ \left\lVert v_i \right\rVert=1$, 则称为标准正交基（orthonormal）.

当 $v_1, \dots, v_n$ 是正交基时，基的线性组合 $v = a_1v_1+\dots+a_nv_n$ 有如下性质：

$$
\begin{align}\left\lVert v \right\rVert^2 &= \left\lVert a_1v_1+\dots+a_nv_n \right\rVert^2\\&=(a_1v_1+\dots+a_nv_n)\cdot (a_1v_1+\dots+a_nv_n)\\&=\sum_{i=1}^n\sum_{j = 1}^n a_ia_j(v_i\cdot v_j)\\&=\sum_{i=1}^n a_i^2\left\lVert v_i \right\rVert^2\quad \text{since}\ v_i \cdot v_j = 0\ \text{for}\ i\neq j \end{align}\\
$$


若基底是标准正交基，则上式可化简为 $\left\lVert v \right\rVert^2 = \sum a_i^2$.

Gram-Schmidt 算法可以创造一个标准正交基，这里讨论通用算法的一个变体，相应给出的是正交基。



**Theorem 7.13 (Gram-Schmidt Algorithm).** 令 $v_1, \dots, v_n$ 是向量空间 $V\subset \mathbb{R}^m$ 的一组基。下面算法可以构建 $V$ 的一个标准正交基 $v_1^*, \dots, v_n^*$：

![image-20250923112529456](https://cdn.jsdelivr.net/gh/hxd77/BlogImage/Blog/image-20250923112529456.png)

图 5. Gram-Schmidt算法，源自《An Introduction to Mathematical Cryptography》


两个基底满足以下性质：

$\text{Span}\{v_1,\dots,v_i\} = \text{Span}\{v_1^*,\dots,v_i^*\}\quad \text{for all}\ i=1,\dots,n.\\$

> 注意对于格 $L$ 来说，Gram-Schmidt 算法生成的 $n$ 个正交基并不一定仍在 $L$ 中，即并不一定仍是格 $L$ 的基。这组基是由 $v_1, \dots, v_n$ 扩张成的向量空间的一组正交基。



_Proof._ 我们需要证明两件事：

-   新生成的基底相互正交
-   两组基底的扩张是相同的

1.  基的正交性利用**数学归纳法**来证明。假设 $v_1^*,\dots, v_{i-1}^*$ 是相互正交的，我们需要证明 $v_i^*$ 与前面所有带有"\*"的向量是正交的。对于 $k<i$（即$k\in [1,i-1]$）, 我们计算：  
    $$
    \begin{align}v_i^* \cdot v_k^* &= \left(v_i - \sum_{j=1}^{i-1}\mu_{ij}v_j^* \right) \cdot v_k^*\\&=v_i \cdot v_k^* - \mu_{ij} \left\lVert v_k^* \right\rVert^2 &\quad \text{since}\ v_k^* \cdot v_j^* = 0 \ \text{for}\ j \neq k,\\&=0 &\quad \text{将}\ \mu_{ij}=v_i\cdot v^*_j/\lVert v_j^*\rVert^2 \quad \text{for} \quad 1\le j<i代入即可得到\end{align}\\
    $$
    
2.  扩张本质是集合，而证明集合的相等我们只需证明**相互包含**即可。

-   $\subseteq$：根据 $v_i^*$ 的定义，我们可以将 $v_i$ 表示为：  
    $v_i = v_i^*+\sum_{j=1}^{i-1}\mu_{ij}v_j^*\\$  
    即 $v_i$ 可以被表示为 $v_i^*$ 的线性组合。于是有 $v_i \in \text{Span}\{v_1^*,\dots,v_i^*\}$. 即有 $\subseteq$
-   $\supseteq$：数学归纳法。假设 $v_1^*,\dots,v_{i-1}^*$ 属于 $\text{Span}\{v_1,\dots,v_{i-1}\}$. 我们需要证明 $v_i^{*} \in \text{Span}\{v_1,\dots,v_i\}$. 根据定义 $v_i^*=v_i - \sum_{j=1}^{i-1}\mu_{ij}v_j^*$, 则 $v_i^* \in \text{Span}\{v_1^*,\dots,v_{i-1}^*, v_i\}$（可以被线性表示）. 而 $v_1^*,\dots,v_{i-1}^*$ 属于 $\text{Span}\{v_1,\dots,v_{i-1}\}$，因此 $v_i^* \in \text{Span}\{v_1,\dots,v_{i-1}, v_i\}$，即有 $\supseteq$